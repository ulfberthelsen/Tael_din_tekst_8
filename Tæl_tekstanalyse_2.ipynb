{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tekstanalyse 2: Named entities regognition (NER) og Parts of Speech Tagging (POS) med SpaCy\n",
    "***\n",
    "***\n",
    "Keywords: `context manager`, `named entity recognition`, `NER`, `parts of speech tagging`, `POS`, `WordCloud`\n",
    "\n",
    "Nye Python-udtryk:  `with`, `.set()`, `.sorted()`, `.join()`, `WordCloud()`\n",
    "***\n",
    "***\n",
    "I det følgende skal vi se nærmere på NLP-pakken `SpaCy`, der er indeholder effektive og kraftfulde redskaber til tekstanalyse. I modsætning til `NLTK`-pakken, der er regelbaseret, er SpaCy baseret på maskinlæring, hvilket bl.a. betyder, at den virker godt på dansk, for sprogmodulet også er trænet på danske tekster. \n",
    "\n",
    "SpaCy kan mange forskellige ting. Vi lægger ud med at kigge på **Named Entity Recognition** (NER) og **Parts of Speech Tagging**.\n",
    "\n",
    "Nogle af elementerne vil være repetition af elementer fra tidligere notebooks.\n",
    "\n",
    "Hvis der er kode sekvenser eller udtryk i ikke forstår, er det altid en god idé at bruge Google. Det kan give svar på det meste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Forberedelse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "Som altid begynder vi med at importere de nødvendige `libraries`. Udover `os`, `Numpy` og `Pandas` skal vi også bruge `matplotlib` og `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                       # os tillader os bl.a. at finde filplaceringer på computeren\n",
    "import numpy as np              # Numpy leverer noget af matematikken, der ligger under Pandas \n",
    "import pandas as pd             # Pandas tillader os at importere, oprette og manipulere data frames\n",
    "import matplotlib.pyplot as plt # Importerer underbiblioteket pyplot fra pakken matplotlib\n",
    "from nltk.text import Text      # nltk indholder mange forskellige funktioner, der kan bruges til tekstanalyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herudover skal vi også importere `SpaCy`. Når vi bruger spacy, skal vi udover at importere modulet, også loade en sprogmodel. Der er tre modeller at vælge mellem: `da_core_news_sm`, `da_core_news_md`, `da_core_news_lg`, en lille (sm), en mellem (md) og en stor (lg). Størrelsen angiver, hvor stort et korpus modellen er blevet trænet på. \n",
    "\n",
    "**Importér** SpaCy og **load** den store danske sprog-model. Den kører derfor lidt langsommere end de andre, men er til gengæld mere præcis. Skal man arbejde med meget store tekstmængder, kan det være en idé at bruge en mindre model. Det er en afvejning af regnekraft/tid mod præcision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy          \n",
    "nlp = spacy.load(\"da_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import af tekster\n",
    "I denne notebook skal vi arbejde med seks forskellige nytårstaler. To af Mette Frederiksen, to af Lars Løkke Rasmussen og to af Helle Thorning Schmidt. **Placér** de downloadede `.txt`-filer i en mappe i den mappe, hvori i har gemt jeres script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context manager\n",
    "Der er forskellige måder at åbne filer på. Nedenfor finder I et eksempel på en såkaldt `context manager`. En af fordelene ved at bruge en context-manager er, at den automatisk lukker filerne igen efter de er blevet indlæst. \n",
    "\n",
    "Kodesekvensen begynder med et `for loop`, fordi vi skal åbne seks forskellige filer. Herefter følger context-manageren, der læses 'brug kommandoen 'open' til at åbne den angivne fil og gem den under variabelnavnet 'f' (f for fil er standard-notation, men det er en variabel, så navnet er valgfrit)'.\n",
    "\n",
    "Herefter tilføjer vi talerne til listen 'Taler'. **Bemærk** at rensning foregår i samme ombæring med `.replace()`. Teksten i .txt-filerne er allerede nogenlunde rensede, og det er derfor ikke nødvendigt at bruge rense-funktionerne. Denne måde virker bedst, hvis det kun er få ting, der skal gøres. Er rensningen mere omfattende, er det mere overskueligt, at definere en pipeline-funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taler = [] # opretter tom liste\n",
    "\n",
    "for fil in os.scandir(r'.\\Taler'): # for-loop\n",
    "    with open (fil, encoding = \"utf8\") as f: # context manager\n",
    "        taler.append(f.read().replace(\"\\n\",\" \").replace(\"*\",\" \")) # tilføj renset tekst til liste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For overskuelighedens skyld, lægger vi ud med at afprøve SpaCy på en enkelt tale. Vi gemmer talen under variabel-navnet 'tale_1'.\n",
    "\n",
    "**Bemærk** notationen `taler[0]`. **Hvad** betyder det, og **hvordan** læses det?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tale_1 = taler[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi har allerede importeret `SpaCy` og load'et den store danske sprogmodel. Modellen blev gemt under navnet `nlp`. Det er standardnotation, men betegnelsen er valgfri, og I kan ændre det, hvis det er nødvendigt, fx hvis I har load'et flere forskellige modeller.\n",
    "\n",
    "Bag navnet 'nlp' gemmer der sig nu en masse funktionalitet, på samme måde som vi tidligere har pakket funktionalitet i vores pipeline-funktioner. Når I anvender 'nlp' på en tekst, gør den derfor mange ting på en gang, og outputtet, som vi gemmer under navnet 'doc1' er et komplekst objekt, der udover teksten indholder en masse tags og meta-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(tale_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi har ikke umiddelbart adgang til metainformationen. Prøv at taste `doc1` og `print(doc1)` i feltet nedenfor, og **diskutér** hvad I ser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Named Entity Regonition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det første vi skal se på er `named entity recognition`, som er en funktion, der gør det muligt at genkende personer, organisationer, steder mm.\n",
    "\n",
    "For at hente informationen ud af vore `doc`-objekt, skal vi bruge `.ents`-kommandoen. Der er nemmest at håntere, hvis vi gemmer informationen på en liste.\n",
    "\n",
    "**Afprøv** sekvensen nedenfor. **Læs** koden og **diskutér**, hvad de enkelte dele betyder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents1 = list(doc1.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis vi bare printer indholdet af variablen 'ents1', får vi blot en ordliste. Vi er derfor nødt til specifikt at bede om de enkelte elementer vha. `.text` og `.label_`-kopmmandoerne.\n",
    "\n",
    "**Afprøv** sekvensen nedenfor. **Læs** koden og **diskutér**, hvad de enkelte dele betyder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in ents1:\n",
    "    print(w.text,w.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personer\n",
    "På listen ovenfor har vi en komplet oversigt over de ord, som SpaCy har vurderet betegner personer, steder osv. Når I skal bruge det analytiske, skal i naturligvis tjekke, om der er fejl. SpaCy performer godt, men der er pt. ingen modeller, der kan levee 100% præcision.\n",
    "\n",
    "Hvis vi vil udtrække en liste over personer, kan vi bruge `.label_` til at sortere med.\n",
    "\n",
    "I eksemplet nedenfor bruger vi `list comprehension`til at lave en liste, samt `.set()` til at slette dubletter, og `.sorted()` til at sortere outputtet. Et `set` (en mængde) er karakteriseret ved, at den kun indholde et eksemplar af hvert element. Når vi trnasformere en liste til en mængde, sletter vi derfor automatisk alle ord-dubletter. Kommandoen `.sorted()`transformerer herefter mængden til en sorteret liste.\n",
    "\n",
    "Som vi så sidst fungerer `list comprehension` lidt som et `for loop`. Vi laver altså en liste af de tekstelementer (.text)fra listen 'ents1', der har et label (.label_), der er lig med \"PER\". Herefter slettes dubletter, og listen sorteres alfabetisk.\n",
    "\n",
    "**Afprøv** kode sekvensen nedenfor. **Læs** koden og **diskutér** hvad de enkelte betyder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personer = sorted(set([t.text for t in ents1 if t.label_ == \"PER\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print** listen for at se resultatet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opgave 1\n",
    "Lav tilsvarende lister, hvor i sorterer efter `\"LOC\"` (steder) og `\"MISC\"` (miscellaneous/diverse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diverse (miscellaneous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opgave 2\n",
    "Vælg en tale af henholdsvis Lars Løkke Rasmussen og Mette Frederiksen og lav for hver af de to taler en NER-analyse samt lister med personer, steder og diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lars Løkke rasmussen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mette Frederiksen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opgave 3\n",
    "**Sammenlign** listerne for de tre taler. **Diskutér** forskelle og ligheder i indholdet af talerne. **Hvordan** kan vi bruge denne information til at karakterisere teksterne.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Parts of Speech Tagging (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det næste vi skal se på er SpaCy's POS-modul. `Parts of Speech Tagging` er en funktion, der gør det muligt at opmærke alle ord med orklasse-tags. \n",
    "\n",
    "For at hente informationen ud af vore doc-objekt, skal vi bruge `.sents`-kommandoen. Der er ligesom sidst nemmest at håndtere, hvis vi gemmer informationen på en liste.\n",
    "\n",
    "**Afprøv** sekvensen nedenfor. **Læs** koden og **diskutér** hvad de enkelte dele betyder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents1 = list(doc1.sents)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hvad** er konsekvensen af at tilføje `[1:]`. **Lav** eventuelt en liste, hvor I udlader dette. Sammenlign de to lister. **Hvad** er forskellen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som vi gjorde ovenfor, kan vi hente tekst og tag ud af listen vha. `.text`og `.pos_`.\n",
    "\n",
    "Kodesekvensen nedenfor giver os en koplet liste (og den er lang).\n",
    "\n",
    "**Afprøv** sekvensen nedenfor. **Læs** koden og **diskutér**, hvad de enkelte dele betyder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for s in sents1:\n",
    "    for w in s:\n",
    "        print(w.text, w.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Substantiver\n",
    "Vi kan lave en en liste over substantiver ved at sortere efter POS-tagget \"NOUN\".\n",
    "\n",
    "Kodesekvensen nedenfor indeholde ene kendte elementer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subst_1 = []\n",
    "for s in sents1:\n",
    "    for w in s:\n",
    "        if w.pos_ == \"NOUN\":\n",
    "                subst_1.append(w.text)\n",
    "\n",
    "print(len(subst_1))\n",
    "\n",
    "print(sorted(set(subst_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opgave 4\n",
    "**Omskriv** kodesekvensen ovenfor, så I i stedet for et `for loop` laver en sorteret liste over substantiver ved hjælp af `list comprehension`. I kan bruge koden for NER-listerne som inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opgave 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lav** tilsvarende lister for **verber** og **adjektiver**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. WordCloud\n",
    "Der en mange måder at visualisere tekstdata på. En måderne er at lave en en `word cloud`.\n",
    "\n",
    "For at lave en word-cloud skal vi først importere WordCloud-modulet.\n",
    "\n",
    "Hvis I ikke har modulet installeret kan i køre følgende linje i terminalen (ikke her i Jupyter):\n",
    "\n",
    "`pip install wordcloud`\n",
    "\n",
    "**Importér** WordCloud-modulet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordCloud tager hele strenge som input. Vi skal derfor først have samlet vores liste af substantiver til en samlet `string`. Dette gøres med `.join()`-kommandoen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_string_1 = \" \".join(subst_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herefter kan vi lave en word cloud over listen af substantiver. Koden er relativt overskuelig og let at læse.\n",
    "\n",
    "**Afprøv** sekvensen nedenfor. **Læs** koden og **diskutér**, hvad de enkelte dele betyder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_sub_1 = WordCloud(width = 1200, height = 800,\n",
    "                background_color ='white',\n",
    "                max_words=20,\n",
    "                min_font_size = 10).generate(sub_string_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cloud_sub_1, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opgave 6\n",
    "Hvis I har tid tilovers skal kan I lave substantiver lister og word-clouds for nogle af de andre taler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
